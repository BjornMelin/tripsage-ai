# Example GitHub Actions workflow for TripSage Database Performance Benchmarking
# 
# This file demonstrates how to integrate the performance benchmarking suite
# into your CI/CD pipeline. Copy this to .github/workflows/performance-benchmarks.yml
# to enable automated performance validation.

name: Database Performance Benchmarks

on:
  # Run on performance-related PRs
  pull_request:
    paths:
      - 'tripsage_core/services/infrastructure/**'
      - 'scripts/performance_benchmarks/**'
      - 'scripts/database/**'
      - '.github/workflows/performance-benchmarks.yml'
  
  # Run on push to main for performance monitoring
  push:
    branches: [ main ]
    paths:
      - 'tripsage_core/services/infrastructure/**'
      - 'scripts/performance_benchmarks/**'
  
  # Allow manual workflow dispatch for on-demand testing
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'ci-validation'
        type: choice
        options:
          - 'ci-validation'
          - 'full-validation'
          - 'quick-test'
      scenario:
        description: 'Workload scenario (for quick-test)'
        required: false
        default: 'mixed'
        type: choice
        options:
          - 'read_heavy'
          - 'vector_search'
          - 'mixed'
      duration:
        description: 'Test duration in seconds'
        required: false
        default: '300'
        type: string

env:
  # Database configuration for testing
  POSTGRES_HOST: localhost
  POSTGRES_PORT: 5432
  POSTGRES_DB: tripsage_test
  POSTGRES_USER: postgres
  POSTGRES_PASSWORD: postgres
  
  # Cache configuration
  REDIS_HOST: localhost
  REDIS_PORT: 6379
  
  # Performance benchmark settings
  BENCHMARK_OUTPUT_DIR: ./benchmark_results
  BENCHMARK_TIMEOUT: 1800  # 30 minutes

jobs:
  # Quick validation for PRs
  quick-validation:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"
      
      - name: Install dependencies
        run: |
          uv sync
      
      - name: Setup database
        run: |
          uv run python scripts/database/run_migrations.py
      
      - name: Run quick performance validation
        run: |
          cd scripts/performance_benchmarks
          uv run python run_benchmarks.py quick-test \
            --scenario mixed \
            --optimization full \
            --duration 180 \
            --users 5 \
            --output-dir ${{ env.BENCHMARK_OUTPUT_DIR }}
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: pr-benchmark-results
          path: scripts/performance_benchmarks/${{ env.BENCHMARK_OUTPUT_DIR }}
          retention-days: 7
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            try {
              const resultsPath = 'scripts/performance_benchmarks/${{ env.BENCHMARK_OUTPUT_DIR }}';
              const files = fs.readdirSync(resultsPath);
              const jsonFiles = files.filter(f => f.endsWith('.json'));
              
              if (jsonFiles.length > 0) {
                const latestResult = jsonFiles[0];
                const resultData = JSON.parse(fs.readFileSync(path.join(resultsPath, latestResult), 'utf8'));
                
                const comment = `## ðŸš€ Performance Benchmark Results
            
            **Quick validation completed for this PR**
            
            | Metric | Result |
            |--------|--------|
            | Test Duration | ${resultData.execution_time || 'N/A'}s |
            | Scenarios | ${resultData.custom_scenario?.scenario?.scenario || 'mixed'} |
            | Status | âœ… Completed |
            
            ðŸ“Š [View detailed results in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
                
                github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: comment
                });
              }
            } catch (error) {
              console.log('Could not post benchmark results:', error.message);
            }

  # Full validation for main branch and manual dispatch
  full-validation:
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"
      
      - name: Install dependencies
        run: |
          uv sync
      
      - name: Setup database
        run: |
          uv run python scripts/database/run_migrations.py
      
      - name: Determine benchmark type
        id: benchmark-type
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "type=${{ github.event.inputs.benchmark_type }}" >> $GITHUB_OUTPUT
            echo "scenario=${{ github.event.inputs.scenario }}" >> $GITHUB_OUTPUT
            echo "duration=${{ github.event.inputs.duration }}" >> $GITHUB_OUTPUT
          else
            echo "type=ci-validation" >> $GITHUB_OUTPUT
            echo "scenario=mixed" >> $GITHUB_OUTPUT
            echo "duration=300" >> $GITHUB_OUTPUT
          fi
      
      - name: Run CI validation
        if: steps.benchmark-type.outputs.type == 'ci-validation'
        run: |
          cd scripts/performance_benchmarks
          uv run python run_benchmarks.py ci-validation \
            --output-dir ${{ env.BENCHMARK_OUTPUT_DIR }}
      
      - name: Run full validation
        if: steps.benchmark-type.outputs.type == 'full-validation'
        run: |
          cd scripts/performance_benchmarks
          uv run python run_benchmarks.py full-validation \
            --output-dir ${{ env.BENCHMARK_OUTPUT_DIR }} \
            --timeout ${{ env.BENCHMARK_TIMEOUT }} \
            --verbose
      
      - name: Run quick test
        if: steps.benchmark-type.outputs.type == 'quick-test'
        run: |
          cd scripts/performance_benchmarks
          uv run python run_benchmarks.py quick-test \
            --scenario ${{ steps.benchmark-type.outputs.scenario }} \
            --optimization full \
            --duration ${{ steps.benchmark-type.outputs.duration }} \
            --users 10 \
            --output-dir ${{ env.BENCHMARK_OUTPUT_DIR }}
      
      - name: Check validation results
        id: validation-check
        run: |
          cd scripts/performance_benchmarks
          if [ -f "${{ env.BENCHMARK_OUTPUT_DIR }}/ci_results.json" ]; then
            success=$(cat ${{ env.BENCHMARK_OUTPUT_DIR }}/ci_results.json | jq -r '.success')
            success_rate=$(cat ${{ env.BENCHMARK_OUTPUT_DIR }}/ci_results.json | jq -r '.high_confidence_claims_met // 0')
            total_claims=$(cat ${{ env.BENCHMARK_OUTPUT_DIR }}/ci_results.json | jq -r '.total_high_confidence_claims // 1')
            
            echo "success=$success" >> $GITHUB_OUTPUT
            echo "success_rate=$success_rate" >> $GITHUB_OUTPUT
            echo "total_claims=$total_claims" >> $GITHUB_OUTPUT
            
            if [ "$success" != "true" ]; then
              echo "âŒ Performance validation failed"
              exit 1
            else
              echo "âœ… Performance validation passed"
            fi
          else
            echo "âš ï¸ No CI results found, assuming success"
            echo "success=true" >> $GITHUB_OUTPUT
          fi
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-results-${{ github.sha }}
          path: scripts/performance_benchmarks/${{ env.BENCHMARK_OUTPUT_DIR }}
          retention-days: 30
      
      - name: Upload performance reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-reports-${{ github.sha }}
          path: scripts/performance_benchmarks/${{ env.BENCHMARK_OUTPUT_DIR }}/*.html
          retention-days: 30
      
      - name: Post results summary
        if: always()
        run: |
          echo "## ðŸ“Š Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark Type | ${{ steps.benchmark-type.outputs.type }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Validation Success | ${{ steps.validation-check.outputs.success || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Claims Met | ${{ steps.validation-check.outputs.success_rate || 'N/A' }}/${{ steps.validation-check.outputs.total_claims || 'N/A' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Commit | ${{ github.sha }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ Detailed results available in workflow artifacts" >> $GITHUB_STEP_SUMMARY

  # Performance monitoring (weekly)
  performance-monitoring:
    if: github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.13'
      
      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          version: "latest"
      
      - name: Install dependencies
        run: |
          uv sync
      
      - name: Setup database
        run: |
          uv run python scripts/database/run_migrations.py
      
      - name: Run comprehensive monitoring
        run: |
          cd scripts/performance_benchmarks
          uv run python run_benchmarks.py full-validation \
            --output-dir ./monitoring_$(date +%Y%m%d_%H%M%S) \
            --timeout 5400 \
            --verbose
      
      - name: Upload monitoring results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-monitoring-${{ github.run_number }}
          path: scripts/performance_benchmarks/monitoring_*
          retention-days: 90

# Scheduled run (optional - uncomment to enable weekly monitoring)
# on:
#   schedule:
#     # Run every Sunday at 2 AM UTC
#     - cron: '0 2 * * 0'